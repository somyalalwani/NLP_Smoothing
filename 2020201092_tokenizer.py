# -*- coding: utf-8 -*-
"""nlp a1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BBk5_uBYrarFnHKUYQyXoh65e3WsIbsv
"""

def word_tokeniser(line):
  words = line.replace("'", " ").split(" ")
  l=[]
  for word in words:
    if word.startswith("#"):   #hashtag
      l.append("<HASHTAG>")
    elif word.startswith("@"): #mentions
      l.append("<MENTION>")
    elif word.startswith("http"): #url
      l.append("<URL>")
    elif word.startswith("www"): #url
      l.append("<URL>")
    elif not(any(c.isalpha() for c in word)): # ->>> to ->
      new_word=""
      if len(word)>0:
        new_word+=word[0]
      if len(word)>1:
        for i in range(1,len(word)):
          if word[i]!=word[i-1]:
            new_word+=word[i]
      l.append(new_word)
    else:                    # hi!!!! to hi !
      new_word=""
      flag=0
      if len(word)>0:
        new_word+=word[0]
        if(word[0].isalpha()):
          flag=1
        else:
          flag=2
      if len(word)>1:
        for i in range(1,len(word)):
          if word[i].isalpha():
            if flag==1:
              new_word+=word[i]
            else:
              l.append(new_word)
              new_word=""
              new_word+=word[i]
              flag=2
          else:
            if flag==1:
              l.append(new_word)
              new_word=""
              new_word+=word[i]
              flag=2
            else:
              if word[i]!=word[i-1]:
                new_word+=word[i]
      l.append(new_word)
  try:
    l.remove('')
  except:
    pass
  return l

with open("/content/general-tweets.txt") as f1:
    with open("/content/2020201092_tokenizer.txt", "w") as f2:
      for line in f1:
        token_of_line = word_tokeniser(line)
        #print(token_of_line)
        f2.write(' '.join([str(elem) for elem in token_of_line]))
f2.close()
f1.close()